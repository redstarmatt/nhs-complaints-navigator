# Comprehensive Risk Analysis

## Public Body Complaints Navigator

**Date:** 30 January 2026
**Version:** 1.0
**Status:** For review

---

## Executive Summary

This document analyses the risks associated with operating an AI-powered complaints navigation tool for UK citizens. The tool uses Google Gemini AI for conversational intake and letter generation, stores data in browser localStorage, and proxies API calls through a Node.js/Express server. It covers NHS, councils, police, DWP, HMRC, schools, and other public bodies.

The analysis identifies **32 specific risks** across eight categories. The most critical risks relate to: AI hallucination of complaint pathways and legal references; data breach of special category health data; safeguarding failures where users believe the tool has made a report on their behalf; and professional regulation exposure under the Legal Services Act 2007.

---

## Risk Scoring Methodology

| Rating | Likelihood Definition | Impact Definition |
|--------|----------------------|-------------------|
| **Low** | Unlikely to occur; no known precedent | Minor inconvenience; easily corrected |
| **Medium** | Could occur; known precedent exists | Moderate harm; requires intervention to correct |
| **High** | Likely to occur given current design | Significant harm; regulatory action, legal liability, or serious user detriment |
| **Critical** | Almost certain or already occurring | Severe harm; physical danger, life-changing impact, criminal liability |

---

## 1. AI Hallucination / Bad Advice Risks

### Risk 1.1: AI provides wrong complaint pathway

| | |
|---|---|
| **Description** | Gemini directs a user to the wrong body (e.g., LGSCO instead of PHSO), wrong stage (e.g., skipping mandatory reconsideration), or wrong process entirely. The user follows this advice, misses the correct deadline, and loses their right to complain. |
| **Likelihood** | **Medium** -- The app uses a hardcoded `router.js` with correct pathway data for each body type, which reduces dependency on AI for routing. However, the AI must first correctly classify the `bodyType` field in its JSON extraction, and misclassification would route the user entirely wrong. |
| **Impact** | **High** -- A user who misses a 1-month DWP mandatory reconsideration deadline, or a 12-month NHS complaints deadline, may permanently lose their right to challenge the decision. |
| **Existing mitigations** | Pathway logic is hardcoded in `router.js`, not generated by AI. The AI only classifies the body type; the pathway steps, time limits, and contact details come from static data. Time limit warnings are displayed in the summary card. DWP decision vs. service complaint routing is explicitly handled. |
| **Recommended additional mitigations** | (1) Add a "confidence check" where the summary card asks the user to explicitly confirm which public body they are complaining about, with a dropdown showing options. (2) Add a visible disclaimer before the pathway card stating "This pathway is based on what you told us -- please verify these details are correct before acting." (3) Build automated tests that verify routing for each body type with sample fact sets. (4) Consider a secondary AI call to cross-check the bodyType classification. |
| **Real-world example** | Citizens Advice's Caddy AI tool found that AI responses were correct only 80% of the time, requiring supervisor review before any response reached the client. See: [Caddy -- Citizens Advice AI copilot](https://ai.gov.uk/projects/caddy/). |

---

### Risk 1.2: AI provides wrong time limits or deadlines

| | |
|---|---|
| **Description** | The AI tells a user they have 12 months when they actually have 1 month (e.g., DWP mandatory reconsideration), or states a deadline has passed when it has not, or vice versa. |
| **Likelihood** | **Low-Medium** -- Time limits are hardcoded in `router.js` pathway data and displayed from static configuration, not AI-generated text. However, the AI could provide incorrect timing information in its conversational responses before the structured extraction occurs. |
| **Impact** | **Critical** -- Missing the 1-month DWP mandatory reconsideration deadline can permanently bar appeal to a tribunal. Missing NHS/council 12-month limits may end the complaint process. |
| **Existing mitigations** | Time limits come from `router.js` static data. The system prompt instructs the AI to flag timing concerns. The summary card shows `withinTimeLimit` status with colour-coded warnings. The deadline calculator computes specific dates from event dates. |
| **Recommended additional mitigations** | (1) Add a prominent "time-sensitive" banner when DWP decision complaints are detected, emphasising the 1-month deadline. (2) Add a post-intake validation step that cross-references the dateRange against the pathway's time limit and surfaces an explicit warning if the deadline is imminent (within 14 days). (3) Include a disclaimer in conversational responses: "Time limits mentioned here are for general guidance -- always check with the body directly." |

---

### Risk 1.3: AI generates complaint letter with fabricated details

| | |
|---|---|
| **Description** | Gemini adds details the user never mentioned, invents reference numbers, cites non-existent legislation, or attributes statements to the user that they did not make. The user submits this letter, and it contains falsehoods. |
| **Likelihood** | **Medium** -- The letter system prompt (`LETTER_SYSTEM_PROMPT` in `chat.js`) explicitly prohibits embellishment and fabrication, with a banned-phrases list and instructions to stick to the user's words. However, LLM hallucination is a fundamental architectural property that cannot be fully eliminated through prompting alone. |
| **Impact** | **High** -- A complaint letter containing fabricated claims could: (a) undermine the user's credibility with the body, (b) constitute making a false statement in a formal complaint (potentially a criminal offence), (c) create liability for the tool operator. |
| **Existing mitigations** | The letter prompt explicitly bans embellishment and lists banned phrases. The prompt instructs "Only include facts they actually told you." The letter is presented in an editable textarea with a usage hint asking users to "Edit the text below to make it your own." A truthfulness notice is displayed in the next-steps panel. |
| **Recommended additional mitigations** | (1) Add a prominent "AI-generated content" watermark/banner on the letter view: "This text was drafted by AI. Please check every detail carefully before sending. You are responsible for the accuracy of any complaint you submit." (2) Add a mandatory "I have reviewed this letter" checkbox before enabling download/copy/email buttons. (3) Consider a fact-checking step where key claims in the letter are compared against the extracted facts JSON to flag potential additions. (4) Add a visible "[AI DRAFT]" watermark to generated PDFs. |
| **Real-world example** | In **Mata v. Avianca, Inc.** (S.D.N.Y. 2023), lawyers submitted a brief containing six entirely fabricated case citations generated by ChatGPT. The lawyers were fined $5,000 and required to notify every judge falsely named. The case demonstrated that AI hallucination of legal references is a concrete, documented risk. See: [Mata v. Avianca -- Wikipedia](https://en.wikipedia.org/wiki/Mata_v._Avianca,_Inc.). |

---

### Risk 1.4: AI cites wrong legislation or ombudsman

| | |
|---|---|
| **Description** | The AI references the wrong Act of Parliament, the wrong ombudsman body, or cites legislation that does not exist or has been repealed. |
| **Likelihood** | **Low-Medium** -- Legislation references are hardcoded in `router.js` pathway data (e.g., "NHS Constitution, Local Authority Social Services and NHS Complaints Regulations 2009"). The letter prompt includes the pathway legislation. However, the AI could hallucinate additional legislation references in conversational responses. |
| **Impact** | **Medium-High** -- Citing wrong legislation in a complaint letter could undermine the complaint. Citing non-existent legislation would damage the user's credibility and the tool's reputation. |
| **Existing mitigations** | The letter prompt says "Briefly mention the right to complain but do not quote legislation." Legislation references come from static `router.js` data, not AI generation. |
| **Recommended additional mitigations** | (1) Instruct the letter prompt not to cite specific section numbers or case law, only general rights. (2) Add a post-generation check that scans the letter for patterns like "Section \d+" or Act references and flags them for user review. (3) Maintain a whitelist of valid legislation references that the AI is permitted to include. |

---

### Risk 1.5: AI tells user they have rights they do not have

| | |
|---|---|
| **Description** | The AI tells a user they have a right to compensation, a right to an oral hearing, or a right to legal aid when they do not. Conversely, it may fail to mention rights they do have (e.g., right to advocacy, right to reasonable adjustments). |
| **Likelihood** | **Medium** -- The system prompt does not exhaustively define rights for each pathway. The AI draws on its training data, which may be outdated or inaccurate for UK-specific complaint rights. |
| **Impact** | **High** -- A user who believes they have a right to compensation may have unrealistic expectations and make claims that damage their complaint. A user who is not told about their right to advocacy may navigate the process unsupported. |
| **Existing mitigations** | The system prompt says "Never give legal advice, but explain rights where relevant." The pathway cards include tips mentioning advocacy (Healthwatch, Citizens Advice) and relevant legislation. |
| **Recommended additional mitigations** | (1) Add a standard "Know your rights" section to each pathway card with verified rights (right to advocacy, right to reasonable adjustments, right to written response, etc.). (2) Add a disclaimer to all conversational responses: "This tool provides general guidance, not legal advice. For advice about your specific legal rights, contact Citizens Advice (0800 144 8848) or a solicitor." (3) Consider periodic human review of AI conversation logs (with consent) to identify patterns of incorrect rights information. |

---

### Risk 1.6: Disclaimers are insufficient to avoid liability

| | |
|---|---|
| **Description** | The tool includes disclaimers, but they may not be legally sufficient to avoid liability if a user suffers harm from following incorrect AI-generated advice. |
| **Likelihood** | **Medium** -- The consent gate requires acknowledgment of three items, but none explicitly state that the AI may provide incorrect information. No terms of service or limitation of liability are presented. |
| **Impact** | **High** -- Under the principle established in **Moffatt v. Air Canada** (2024 BCCRT 149), the operator cannot disclaim liability simply by pointing to disclaimers elsewhere on the site. The tribunal held that a user is entitled to rely on information presented by a chatbot and has no duty to cross-check it against other sources. |
| **Existing mitigations** | Consent gate with three checkboxes. Privacy notice. Truthfulness notice in the next-steps panel. Letter editor encourages users to edit the text. |
| **Recommended additional mitigations** | (1) Add comprehensive Terms of Service that explicitly state: the tool provides general information only, not legal advice; AI may produce inaccurate information; the user is responsible for verifying all information; the operator accepts no liability for decisions made based on AI output. (2) Add an explicit disclaimer to the consent gate: "AI can make mistakes. You should check any information or guidance before acting on it." (3) Include a visible "This is AI-generated guidance, not legal advice" banner on every screen. (4) Seek legal advice on whether the current disclaimers are sufficient under English law. |
| **Legal references** | **Moffatt v. Air Canada** (2024 BCCRT 149): Tribunal held Air Canada liable for chatbot's inaccurate bereavement fare information, rejecting the argument that the chatbot was a separate entity. See: [BC Tribunal Confirms Companies Remain Liable](https://www.americanbar.org/groups/business_law/resources/business-law-today/2024-february/bc-tribunal-confirms-companies-remain-liable-information-provided-ai-chatbot/). While this is a Canadian case, the principle -- that an operator is liable for information provided by its AI tools -- is likely to apply in UK negligence law under the duty of care established in **Caparo v. Dickman** [1990] 2 AC 605 (proximity, foreseeability, and fairness). |

---

### Risk 1.7: SRA and Law Society position on AI-generated legal guidance

| | |
|---|---|
| **Description** | The SRA or Law Society could take enforcement action or issue critical public statements about the tool, particularly if it is seen as providing legal advice without authorisation. |
| **Likelihood** | **Low** -- The SRA has shown a pro-innovation stance, authorising the first "purely AI-based firm" (Garfield.Law) in May 2025. However, Garfield.Law operates under SRA regulation with named solicitors accountable. An unregulated tool without professional oversight faces different scrutiny. |
| **Impact** | **Medium** -- Negative SRA commentary could damage reputation and trigger regulatory attention. However, since the tool does not perform reserved legal activities (see Risk 4.1), the SRA's direct regulatory authority is limited. |
| **Existing mitigations** | The system prompt states "Never give legal advice." The tool describes itself as providing "guidance" and "information." |
| **Recommended additional mitigations** | (1) Engage proactively with the SRA's innovation team to discuss the tool's approach. (2) Consider seeking informal SRA guidance on whether the tool's activities could be construed as providing legal services. (3) Ensure all marketing and user-facing copy consistently uses "information" and "guidance" rather than "advice." |
| **Reference** | SRA Risk Outlook report on AI in the legal market: [SRA AI Risk Outlook](https://www.sra.org.uk/sra/research-publications/artificial-intelligence-legal-market/). SRA approval of Garfield.Law: [SRA OKs 1st AI Law Firm](https://www.financialinstitutionslegalsnapshot.com/2025/05/08/uks-solicitors-regulatory-authority-authorises-first-purely-ai-based-firm/). |

---

## 2. Data Breach Risks

### Risk 2.1: Health data breach via Gemini API (Google side)

| | |
|---|---|
| **Description** | Health and disability data is sent to Google's Gemini AI for processing. If Google's systems are breached, or if data is accessed by Google employees contrary to the Data Processing Addendum, special category health data could be exposed. |
| **Likelihood** | **Medium** -- Google's Gemini platform has experienced multiple security incidents in 2024-2025. The "GeminiJack" exploit (June 2025) allowed exfiltration of corporate data from Google Workspace via indirect prompt injection. A Gemini AI data leak (February 2024) saw chat prompts indexed by search engines. The Gemini CLI vulnerability (July 2025) allowed malicious code execution. |
| **Impact** | **Critical** -- Health data is special category data under UK GDPR Article 9. A breach would almost certainly trigger both Article 33 (ICO notification within 72 hours) and Article 34 (individual notification) obligations. ICO fines for health data breaches have been substantial: Advanced Computer Software Group was fined GBP 3.07 million (March 2025) for a ransomware breach affecting NHS data. Capita was fined GBP 14 million (October 2025) for a breach affecting 6.6 million people. |
| **Existing mitigations** | NHS numbers and NI numbers are redacted before sending to Gemini (via `redactIdentifiers()` in `chat.js`). The complaint narrative (which may contain health details) is sent in full. Server-side proxy prevents direct client-API communication. |
| **Recommended additional mitigations** | (1) **Upgrade to paid Gemini API tier or Vertex AI immediately.** Google's terms state that unpaid/free-tier data may be used for model training and reviewed by humans. The paid tier is governed by the Cloud Data Processing Addendum. Google's Gemini API terms also require paid services for UK-serving applications. (2) Accept the Google Cloud Data Processing Addendum (CDPA) in the Google Cloud console. (3) Enable UK data residency where available. (4) Implement a data processing agreement log. (5) Consider anonymising the narrative before API submission (e.g., replacing names with placeholders). (6) Monitor Google's security advisories for Gemini-related incidents. |
| **Legal references** | UK GDPR Articles 33-34: 72-hour notification to ICO; notification to affected individuals for high-risk breaches. ICO guidance: [Personal Data Breaches Guide](https://ico.org.uk/for-organisations/report-a-breach/personal-data-breach/personal-data-breaches-a-guide/). Advanced Computer Software fine: [NHS processor fined GBP 3m](https://www.pinsentmasons.com/out-law/news/nhs-processor-fined-ransomware-data-breach). Capita fine: [ICO Fines Capita GBP 14 Million](https://www.insideprivacy.com/data-privacy/ico-fines-capita-14-million-over-2023-data-breach/). Gemini security incidents: [Google Patches AI Flaw](https://www.cuinfosecurity.com/google-patches-ai-flaw-that-turned-gemini-into-spy-a-30236). |

---

### Risk 2.2: Health data breach via browser localStorage

| | |
|---|---|
| **Description** | Complaint data (including health information, disability details, safeguarding concerns) is stored in plaintext in browser localStorage. On a shared computer, public terminal, or compromised device, this data is accessible to any user or malware with access to the browser. |
| **Likelihood** | **High** -- Many people who need to complain about public services may use shared computers (libraries, community centres, family devices). localStorage is accessible to any JavaScript running on the same origin, and to anyone with physical or remote access to the browser's developer tools or file system. |
| **Impact** | **High** -- Exposure of health data, safeguarding concerns, and complaint narratives could cause significant distress and harm, particularly for vulnerable users. |
| **Existing mitigations** | Auto-expiry after 30 days (`SESSION_MAX_AGE_MS`). User can delete all data via "Delete all my data" button. Data minimisation: full conversation history is dropped after fact extraction; only structured facts are retained. Maximum 10 sessions stored. |
| **Recommended additional mitigations** | (1) **Implement client-side encryption** (task #16 in existing backlog). Use a user-provided passphrase or the Web Crypto API with a key derived from a session token. (2) Add a prominent warning on shared/public computers: "If you are using a shared computer, please use the 'Delete all my data' button before leaving." (3) Consider offering a "no save" mode where nothing is persisted to localStorage. (4) Add a warning in the privacy notice about shared computer risks. (5) Reduce session auto-expiry from 30 days to 7 days for unencrypted storage. |

---

### Risk 2.3: Breach notification obligations

| | |
|---|---|
| **Description** | If any breach occurs (Google side, server side, or user device), the operator has legal obligations under UK GDPR Articles 33 and 34 to notify the ICO within 72 hours and notify affected individuals without undue delay if the breach is high-risk. |
| **Likelihood** | **Medium** -- Any breach involving health data is almost certain to meet the threshold for both ICO and individual notification, given the special category nature of the data. |
| **Impact** | **Critical** -- Failure to notify can result in fines of up to GBP 8.7 million or 2% of annual turnover. The ICO's 2025 enforcement trend shows a shift towards larger, more targeted fines (average fine increased from GBP 150,000 to over GBP 2.8 million in 2025). |
| **Existing mitigations** | Server logs sanitise error messages to strip personal data (`sanitiseError()` in `server.js`). No persistent server-side storage of personal data reduces the server breach surface. |
| **Recommended additional mitigations** | (1) Create a documented breach response plan covering: detection, assessment, containment, ICO notification (within 72 hours), individual notification, and post-incident review. (2) Maintain a breach register as required by Article 33(5). (3) Identify who is responsible for breach notification (data controller contact details are currently "[INSERT]" in the privacy notice). (4) Register with the ICO and pay the data protection fee (task #9 in existing backlog). (5) Set up monitoring/alerting for Google Gemini security advisories. |
| **Legal reference** | ICO breach notification guidance: [UK GDPR data breach reporting](https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/personal-data-breaches/). ICO enforcement trends 2025: [ICO Enforcement in 2025](https://measuredcollective.com/ico-enforcement-in-2025-record-fines-and-what-they-mean/). |

---

### Risk 2.4: Email transmission of unencrypted health data

| | |
|---|---|
| **Description** | When a user sends their complaint letter by email via the tool, the letter (containing health data) is transmitted via Gmail SMTP. The data is stored on Gmail servers and the recipient's mail server, neither of which are under the operator's control. |
| **Likelihood** | **Medium** -- Only triggered when the user actively chooses to send by email. However, if the tool encourages email sending as a primary option, many users may use it without understanding the implications. |
| **Impact** | **Medium-High** -- Email is not end-to-end encrypted. Gmail-to-Gmail uses TLS in transit, but delivery to the public body's mail server depends on their configuration. Health data in the email body and PDF attachment could be intercepted or stored insecurely. |
| **Existing mitigations** | Email sending is optional; users can download and send manually instead. The feature is only available if the server is configured with email credentials. |
| **Recommended additional mitigations** | (1) Add a clear warning before email send: "Email is not fully encrypted. Your complaint may contain sensitive health information. Consider downloading and submitting via the body's secure online portal instead." (2) Review Gmail SMTP processor arrangement (task #13). Consider Google Workspace or a transactional email service with a proper DPA. (3) Consider removing the email feature entirely and directing users to download and submit through official channels. |

---

## 3. Safeguarding Failure Risks

### Risk 3.1: User believes the tool has reported a crime or made a safeguarding referral

| | |
|---|---|
| **Description** | A user discloses abuse, neglect, or a crime. They believe the tool has reported this to the police or social services, and do not contact these services themselves. A child or vulnerable adult remains at risk. |
| **Likelihood** | **Low** (after current mitigations) -- The system prompt explicitly and repeatedly prohibits the AI from claiming to take action. The safeguarding gate blocks complaint processing for serious concerns and directs users to emergency services. The consent gate requires users to confirm they understand the tool does not make reports. |
| **Impact** | **Critical** -- If a child or vulnerable adult is harmed because the user believed a report had been made, the consequences could include serious injury, death, and catastrophic reputational and legal liability for the operator. |
| **Existing mitigations** | Extensive safeguarding protocol in the system prompt (Priorities 1-4) with explicit prohibitions on claiming to take action. Safeguarding gate (`showSafeguardingGate()` in `chat.js`) blocks complaint processing for emergency, crime, child safeguarding, and adult safeguarding concerns, and displays emergency contact numbers. For serious concerns, the tool sends the user back to the home screen rather than continuing. Consent gate checkbox: "I understand this tool does not report crimes or make safeguarding referrals on my behalf." |
| **Recommended additional mitigations** | (1) Add automated testing with adversarial prompts that describe abuse scenarios, verifying the AI never claims to report. (2) Consider a periodic audit of AI responses (with user consent) to check for safeguarding response failures. (3) Add a persistent safeguarding banner visible at all times (not just after detection) with text like: "If someone is in danger, call 999. This tool cannot contact emergency services." (4) Log (anonymised) safeguarding trigger events to monitor frequency and response quality. (5) Consider adding a "Report a concern" link that opens an external safeguarding contact page. |
| **Legal context** | While no UK case has directly established a duty of care for a digital tool that handles safeguarding disclosures, the **Online Safety Act 2023** creates statutory duties for online platforms to protect children and take action against illegal content, with fines of up to GBP 18 million or 10% of turnover. More broadly, a negligence claim could be brought under the **Caparo** framework if a court found sufficient proximity between the tool operator and the harmed individual. The **Children Act 1989** and **Care Act 2014** impose safeguarding duties on specific bodies, and while these do not directly apply to a private digital tool, operating in the safeguarding space creates reputational and moral obligations. |

---

### Risk 3.2: Inadequate signposting for specific safeguarding scenarios

| | |
|---|---|
| **Description** | The AI correctly identifies a safeguarding concern but provides wrong or incomplete signposting (e.g., wrong phone number, wrong agency, or does not account for devolved nations where safeguarding structures differ in Scotland, Wales, and Northern Ireland). |
| **Likelihood** | **Medium** -- The system prompt provides specific phone numbers for England-based services (NSPCC, CQC, GMC, NMC). Devolved nations have different safeguarding structures (e.g., Care Inspectorate Wales, Social Care and Social Work Improvement Scotland). These are not covered in the prompt. |
| **Impact** | **High** -- Wrong signposting could delay a safeguarding referral. While the user would likely find the correct service through secondary research, any delay in safeguarding can have serious consequences. |
| **Existing mitigations** | Safeguarding gate provides emergency numbers (999, 101, NSPCC helpline 0808 800 5000). System prompt includes contacts for CQC, GMC, NMC. Postcode lookup determines which nation the user is in. |
| **Recommended additional mitigations** | (1) Add nation-specific safeguarding contacts for Wales, Scotland, and Northern Ireland in both the system prompt and the safeguarding gate. (2) Add a general "I'm not sure where to report" option that directs to 101 as the catch-all. (3) Maintain a verified, regularly updated list of safeguarding contacts. |

---

## 4. Professional Regulation Risks

### Risk 4.1: Providing legal services without authorisation under the Legal Services Act 2007

| | |
|---|---|
| **Description** | Operating the tool could be construed as providing legal services, which could trigger regulatory scrutiny or criminal prosecution under the Legal Services Act 2007. |
| **Likelihood** | **Low** -- The six reserved legal activities under the Legal Services Act 2007 are: exercise of a right of audience, conduct of litigation, reserved instrument activities, probate activities, notarial activities, and administration of oaths. The tool performs none of these. Providing legal advice and legal information are explicitly classified as "unreserved" legal activities under section 12 of the Act, meaning they can be carried out by anyone. |
| **Impact** | **Medium** -- Even though the risk of prosecution is low, being seen as providing unregulated legal advice could trigger SRA investigation, reputational damage, and loss of trust from partner organisations. |
| **Existing mitigations** | System prompt says "Never give legal advice, but explain rights where relevant." The tool describes itself as a "guidance" and "information" tool. |
| **Recommended additional mitigations** | (1) Ensure all user-facing copy, marketing, and metadata use "information" and "guidance" exclusively, never "advice." (2) Add a clear statement: "This tool does not provide legal advice. For legal advice about your specific situation, contact a solicitor or Citizens Advice." (3) Seek formal legal opinion on whether the tool's activities fall within or outside the scope of reserved activities. (4) Consider registering with a relevant quality framework (see Risk 4.3). |
| **Legal reference** | Legal Services Act 2007, sections 12-14: [Reserved legal activities -- Legal Services Board](https://legalservicesboard.org.uk/enquiries/frequently-asked-questions/reserved-legal-activities). SRA guidance on non-reserved legal activity: [SRA MDPs guidance](https://www.sra.org.uk/solicitors/guidance/non-reserved-legal-activity/). |

---

### Risk 4.2: Blurring the line between information and advice

| | |
|---|---|
| **Description** | Although "legal advice" is technically unreserved under the Legal Services Act, there is a practical distinction between generic information ("you have 12 months to complain") and tailored advice ("based on your specific circumstances, you should do X"). The tool provides tailored guidance based on the user's specific facts, which could be characterised as advice rather than information. |
| **Likelihood** | **Medium** -- The tool analyses specific facts, classifies body types, determines complaint pathways, and generates personalised complaint letters. This goes beyond generic information provision. |
| **Impact** | **Medium** -- Could attract regulatory attention from the SRA, the Legal Ombudsman, or consumer protection bodies. The SRA has noted that "cases involving the provision of reserved legal activities do not move between SRA-regulated and other services in a way that causes detriment." |
| **Existing mitigations** | The system prompt prohibits giving "legal advice." The pathway data is static and rule-based (not AI-generated). The letter is presented as a draft for the user to review and edit. |
| **Recommended additional mitigations** | (1) Add a clear, visible "This is not legal advice" banner on every screen. (2) Include in the Terms of Service: "This tool provides general information about complaints processes. It does not analyse the legal merits of your complaint or provide advice on whether you should complain. For advice about your legal rights, consult a qualified adviser." (3) Consider seeking accreditation from AdviceUK or a similar body to demonstrate quality standards. |

---

### Risk 4.3: Non-compliance with advice sector quality standards

| | |
|---|---|
| **Description** | The tool operates in the same space as Citizens Advice, AdviceUK members, and law centres, all of which operate under the Advice Quality Standard (AQS) or equivalent frameworks. Operating without any quality accreditation could undermine trust with partner organisations and users. |
| **Likelihood** | **Medium** -- Not legally required, but absence of accreditation could be raised as a criticism if the tool causes harm. |
| **Impact** | **Low-Medium** -- Reputational risk. Potential barrier to partnerships with councils, NHS bodies, or advice organisations. |
| **Existing mitigations** | None identified. |
| **Recommended additional mitigations** | (1) Investigate AQS accreditation or AdviceUK membership. (2) Adopt AdviceUK's AI products (ASSIST/INFORM) as a reference framework for quality. (3) Engage with Citizens Advice or local Healthwatch to explore partnership or endorsement. (4) Reference the government's Algorithmic Transparency Recording Standards (ATRS) as used by Citizens Advice's Caddy tool. See: [Caddy project](https://ai.gov.uk/projects/caddy/). AdviceUK AI partnership: [AdviceUK AI Solutions](https://www.adviceuk.org.uk/ai-solutions/). |

---

## 5. Accessibility / Equality Act Risks

### Risk 5.1: WCAG 2.1/2.2 AA compliance

| | |
|---|---|
| **Description** | The tool may not meet WCAG 2.1 AA (or the current 2.2 AA) accessibility standards, which are legally required for public sector websites and expected under the Equality Act 2010 for all service providers. |
| **Likelihood** | **Medium-High** -- The codebase uses semantic HTML (`role` attributes, `aria-label`, `aria-live`), but a full WCAG audit has not been conducted. Potential issues include: chat messages using `role="log"` with `aria-live="polite"` (may not announce new messages reliably across all screen readers); dynamic content injection (summary cards, pathway cards) may not be announced; colour contrast has not been verified; focus management during view transitions needs testing. |
| **Impact** | **High** -- The Equality Act 2010 requires reasonable adjustments. The Public Sector Bodies Accessibility Regulations 2018 require WCAG 2.2 AA compliance for public sector websites (relevant if the tool partners with or is used by public bodies). Non-compliance could result in EHRC enforcement action or discrimination claims. |
| **Existing mitigations** | Semantic HTML with ARIA attributes on key elements. `aria-label` on interactive elements (buttons, inputs). `aria-live="polite"` on chat messages container. `role="dialog"` and `aria-modal="true"` on consent gate and privacy modal. Keyboard-navigable (Enter to send, standard form controls). |
| **Recommended additional mitigations** | (1) Commission a WCAG 2.2 AA accessibility audit. (2) Test with screen readers (NVDA, JAWS, VoiceOver) and keyboard-only navigation. (3) Verify colour contrast ratios meet 4.5:1 for normal text and 3:1 for large text. (4) Ensure focus is properly managed when views switch (landing -> chat -> letter -> diary). (5) Add skip-to-content links. (6) Ensure all dynamically inserted content is announced to screen readers. (7) Publish an accessibility statement as required by the Public Sector Bodies Accessibility Regulations 2018. |
| **Legal reference** | Equality Act 2010: [Understanding accessibility requirements for public sector bodies](https://www.gov.uk/guidance/accessibility-requirements-for-public-sector-websites-and-apps). Public Sector Bodies Accessibility Regulations 2018. WCAG 2.2: [W3C WCAG](https://www.w3.org/TR/WCAG22/). |

---

### Risk 5.2: Users who cannot use a text-based chat interface

| | |
|---|---|
| **Description** | Users with visual impairments, motor disabilities, cognitive disabilities, or low literacy may struggle with a text-based chat interface. The tool's primary interaction mode requires typing and reading. |
| **Likelihood** | **High** -- Many people who need to complain about public services have disabilities or accessibility needs. The tool explicitly asks about accessibility needs during intake. |
| **Impact** | **Medium-High** -- Excluding users who cannot type or read effectively undermines the tool's purpose. Under the Equality Act 2010, service providers must make reasonable adjustments to avoid putting disabled people at a substantial disadvantage. |
| **Existing mitigations** | Voice input (speech-to-text) via the Web Speech API microphone button. Text-to-speech (auto-read toggle) for AI responses. Per-message "Read aloud" buttons. The intake prompt asks about accessibility needs and offers to note preferences. |
| **Recommended additional mitigations** | (1) Add Easy Read content for key information (landing page, pathway descriptions). (2) Consider British Sign Language (BSL) video alternatives for key guidance. (3) Ensure voice input works reliably across devices and browsers (currently depends on `SpeechRecognition` API availability). (4) Add large text / high contrast mode toggle. (5) Provide a phone-based alternative or referral to Citizens Advice telephone service (0800 144 8848) for users who cannot use the web interface. (6) Test with users with accessibility needs. |

---

### Risk 5.3: Public Sector Equality Duty implications

| | |
|---|---|
| **Description** | If the tool is used by or partnered with public bodies (e.g., embedded on an NHS trust website, recommended by a council), the Public Sector Equality Duty (PSED) under Section 149 of the Equality Act 2010 applies. The public body must have due regard to the need to eliminate discrimination, advance equality, and foster good relations. |
| **Likelihood** | **Medium** -- The tool is designed to be used in the context of complaints about public bodies. It is foreseeable that public bodies could link to or recommend the tool. |
| **Impact** | **Medium-High** -- If the tool is inaccessible or biased, a public body that recommends it could face PSED challenges. This could also block partnership opportunities. |
| **Existing mitigations** | The intake prompt asks about accessibility needs and vulnerability flags. The pathway cards include reasonable adjustments reminders. |
| **Recommended additional mitigations** | (1) Conduct an Equality Impact Assessment (EIA) for the tool. (2) Ensure WCAG compliance (see Risk 5.1). (3) Test for bias in AI responses across different demographic groups and complaint types. (4) If partnering with public bodies, provide them with an accessibility statement and DPIA they can reference. |

---

## 6. Operational / Technical Risks

### Risk 6.1: Gemini API downtime or degradation

| | |
|---|---|
| **Description** | Google Gemini API experiences outages, rate limiting, or degraded performance, rendering the tool non-functional. |
| **Likelihood** | **Medium** -- Cloud APIs experience periodic outages. The server already handles 429 (rate limit) and 5xx (server error) responses from Gemini. |
| **Impact** | **High** -- The tool is entirely dependent on Gemini for its core function. No AI means no intake, no fact extraction, and no letter generation. Users with urgent complaints (approaching deadlines) would be unable to use the tool. |
| **Existing mitigations** | Error handling in `server.js` returns user-friendly messages for specific HTTP status codes (400, 403, 429, 5xx). Rate limit message: "Rate limit reached. Please wait a moment." |
| **Recommended additional mitigations** | (1) Add a fallback mode that displays static pathway information when the AI is unavailable, allowing users to at least find the correct complaints process manually. (2) Display estimated downtime or a "service status" indicator. (3) Add retry logic with exponential backoff for transient failures. (4) Monitor Gemini API status and set up alerts. (5) Consider a secondary AI provider (e.g., Anthropic Claude, OpenAI) as a failover. |

---

### Risk 6.2: API cost overruns

| | |
|---|---|
| **Description** | High usage or abuse drives Gemini API costs beyond budget. Each conversation involves multiple API calls (initial greeting, multiple intake turns, letter generation, MP letter generation). |
| **Likelihood** | **Medium** -- Depends on usage volume. Gemini 2.0 Flash pricing is relatively low, but costs scale with conversation length and number of users. Abuse (automated requests, prompt injection) could spike costs. |
| **Impact** | **Medium** -- Unexpected costs could force service suspension or require emergency funding. |
| **Existing mitigations** | Server-side proxy controls API access (API key is not exposed to clients). Request body limit of 1MB (`express.json({ limit: '1mb' })`). |
| **Recommended additional mitigations** | (1) Implement rate limiting per IP address or session. (2) Set up Google Cloud budget alerts and spending caps. (3) Add request logging to track usage patterns. (4) Implement session limits (e.g., maximum 20 messages per session). (5) Add CAPTCHA or other bot protection to prevent automated abuse. |

---

### Risk 6.3: Dependency on free-tier APIs that could change terms

| | |
|---|---|
| **Description** | The tool depends on Postcodes.io (free, no key required) and the UK Parliament Members API (free, no key required). Either could change terms, add rate limits, require authentication, or shut down. |
| **Likelihood** | **Medium** -- Free APIs frequently change terms. Postcodes.io is maintained by Ideal Postcodes Ltd. The Parliament API is maintained by the UK Parliament. |
| **Impact** | **Low-Medium** -- Postcode lookup and MP lookup are non-essential features. The tool functions without them (postcode lookup failure is silently caught). |
| **Existing mitigations** | Both API calls fail silently (`catch` blocks with no error message to the user). The tool continues to function without resolved bodies or MP data. |
| **Recommended additional mitigations** | (1) Cache postcode lookup results server-side to reduce API calls. (2) Add a fallback for MP lookup (e.g., link to TheyWorkForYou.com). (3) Monitor API status and terms changes. (4) Consider adding a commercial postcode API as a backup. |

---

### Risk 6.4: Google discontinues or materially changes Gemini API

| | |
|---|---|
| **Description** | Google has a history of discontinuing products ("Google Graveyard"). The Gemini API could be deprecated, the free tier removed, pricing increased substantially, or the model changed in ways that break the tool's functionality. |
| **Likelihood** | **Medium** -- Google rebranded Bard to Gemini in February 2024 and has iterated rapidly on model versions. The API is labelled "v1beta." API terms changes could occur without notice. Google's terms for UK users already require paid services for UK-serving applications. |
| **Impact** | **High** -- Complete service outage until migrated to an alternative. |
| **Existing mitigations** | The API wrapper in `gemini.js` is a thin abstraction (17 lines of core code), making migration to another provider relatively straightforward. |
| **Recommended additional mitigations** | (1) Abstract the AI layer further so that switching providers requires changing only one file. (2) Evaluate and test alternative providers (Anthropic Claude API, OpenAI, Mistral). (3) Maintain a documented migration plan for each alternative. (4) Monitor Google's API deprecation notices. |

---

### Risk 6.5: Railway hosting reliability

| | |
|---|---|
| **Description** | The tool is hosted on Railway. Railway could experience outages, change pricing, or discontinue its free/starter tier. |
| **Likelihood** | **Low-Medium** -- Railway is a relatively young platform. Its pricing and terms have changed multiple times since launch. |
| **Impact** | **Medium** -- Service unavailability during hosting issues. Migration to another platform (Render, Fly.io, Heroku, AWS) would require some DevOps work but the app is a standard Node.js/Express application. |
| **Existing mitigations** | The app is a standard Express application with no Railway-specific dependencies. |
| **Recommended additional mitigations** | (1) Document a migration plan for alternative hosting platforms. (2) Set up uptime monitoring with alerts. (3) Consider a CDN or static fallback page for when the server is down. |

---

## 7. Reputational Risks

### Risk 7.1: Media reports on AI giving bad complaint advice

| | |
|---|---|
| **Description** | A journalist or campaigner tests the tool and finds it gives incorrect advice, wrong pathways, or generates a complaint letter with fabricated details. This is published as a news story. |
| **Likelihood** | **Medium-High** -- AI tools in public services are under intense media scrutiny. The Guardian, BBC, and others have extensively covered AI failures in DWP's welfare system, NHS data breaches, and AI bias. A complaints tool affecting NHS and DWP users would be a natural target. |
| **Impact** | **High** -- Media coverage could destroy user trust, attract regulatory attention from the ICO and SRA, and force service suspension. The reputational damage could be permanent for a tool that relies on trust. |
| **Existing mitigations** | None specifically for media/reputational risk. |
| **Recommended additional mitigations** | (1) Conduct thorough testing with domain experts (complaints advisers, Citizens Advice staff, solicitors) before public launch. (2) Soft-launch with a beta programme and invite feedback. (3) Prepare a media response plan. (4) Maintain a visible "Report a problem" mechanism. (5) Publish transparency information about how the AI works and its limitations. (6) Be proactive: publish the tool's approach to accuracy and limitations on the website. |
| **Real-world example** | DWP's AI welfare fraud detection system received sustained negative media coverage after a Guardian investigation revealed bias. See: [AI in UK benefits system -- The Conversation](https://theconversation.com/ai-was-supposed-to-make-the-uk-benefits-system-more-efficient-instead-its-brought-bias-and-hunger-245616). |

---

### Risk 7.2: Complaints drafted by the tool are rejected or cause problems

| | |
|---|---|
| **Description** | A complaint letter generated by the tool is rejected by the public body (e.g., because it contains fabricated details, wrong references, or inappropriate language), or the user experiences adverse consequences from submitting it. |
| **Likelihood** | **Medium** -- The letter prompt is well-designed to avoid formal/legal language and stick to facts. However, AI generation always carries a risk of inappropriate content. |
| **Impact** | **Medium-High** -- The user's complaint may be dismissed or taken less seriously. The user may blame the tool. Word-of-mouth damage in communities that most need the service. |
| **Existing mitigations** | Letter is presented as editable text with a usage hint. Truthfulness notice warns users to check facts before sending. Letter prompt explicitly bans embellishment. |
| **Recommended additional mitigations** | (1) Add a post-generation quality indicator (e.g., "This letter is X words -- complaints over 500 words may be less effective"). (2) Add example complaint letters from public bodies' own guidance for comparison. (3) Encourage users to have someone else read the letter before sending. |

---

### Risk 7.3: Tool perceived as replacing human advice services

| | |
|---|---|
| **Description** | Citizens Advice, law centres, advocacy services, or advice sector bodies view the tool as an unregulated competitor that undermines their work or displaces human advisers. |
| **Likelihood** | **Medium** -- The advice sector is under significant funding pressure. An AI tool that appears to offer the same service for free could be seen as a threat. |
| **Impact** | **Medium** -- Loss of potential partnerships. Public criticism from trusted organisations. Advice sector bodies could lobby regulators or media against the tool. |
| **Existing mitigations** | The pathway tips recommend Citizens Advice, Healthwatch, and other human advice services. The intake prompt acknowledges the value of free advocacy support. |
| **Recommended additional mitigations** | (1) Position the tool explicitly as complementary to human advice, not a replacement. (2) Include prominent referrals to Citizens Advice (0800 144 8848), local law centres, and sector-specific advocacy. (3) Engage proactively with Citizens Advice and AdviceUK to explore partnership. Citizens Advice's own Caddy tool uses AI to augment advisers, not replace them -- align messaging with this approach. (4) Consider open-sourcing the tool or offering it to advice organisations (as Citizens Advice did with Caddy). |

---

## 8. Insurance / Liability Risks

### Risk 8.1: Lack of professional indemnity insurance

| | |
|---|---|
| **Description** | The operator has no professional indemnity (PI) insurance to cover claims arising from incorrect advice or guidance provided by the tool. |
| **Likelihood** | **High** -- PI insurance is not currently in place (no evidence of insurance arrangements in the codebase or documentation). |
| **Impact** | **High** -- A single claim from a user who suffered loss due to incorrect guidance (e.g., missed a deadline, lost a tribunal appeal, submitted a fabricated complaint) could result in uninsured liability. |
| **Existing mitigations** | None identified. |
| **Recommended additional mitigations** | (1) Obtain professional indemnity insurance. Note: "Silent AI" is an emerging concern in the insurance market -- many PI policies may not explicitly cover AI-related errors. Specifically declare AI usage to the insurer. (2) Obtain cyber liability insurance covering data breaches, notification costs, and regulatory fines. (3) Ensure the policy explicitly covers AI-generated content and advice. (4) Review policy annually as the AI risk landscape evolves. |
| **Market context** | RSA Insurance's Head of PI acknowledged: "We are likely assuming, but not yet pricing for, Gen AI exposures in our PI book." Average PI fines have increased sevenfold in 2025. Leading AI companies (OpenAI, Google) include limitation of liability clauses that shift responsibility to the business user. See: [Silent AI cover risks](https://www.kennedyslaw.com/en/thought-leadership/article/2025/silent-ai-cover-the-unforeseen-risks-for-insurers/). [AI PI Insurance 2025 Guide](https://whiteoakuk.com/professional-indemnity-insurance-loans-ai-risk-2025/). |

---

### Risk 8.2: No Terms of Service or limitation of liability

| | |
|---|---|
| **Description** | The tool has no Terms of Service. There is no contractual limitation of liability. The consent gate covers data processing consent but does not establish a contractual relationship with limitation clauses. |
| **Likelihood** | **High** -- No Terms of Service exist in the codebase. |
| **Impact** | **High** -- Without Terms of Service, the operator has no contractual basis to limit liability. Any claim would be assessed under common law negligence principles, where liability is potentially unlimited. Under **Moffatt v. Air Canada**, a company was held liable for its chatbot's misrepresentations even where the correct information was available elsewhere on the website. |
| **Existing mitigations** | None. |
| **Recommended additional mitigations** | (1) Draft and implement comprehensive Terms of Service including: clear statement that the tool provides general information, not legal advice; limitation of liability clause (noting that liability for death or personal injury caused by negligence cannot be excluded under the Unfair Contract Terms Act 1977); exclusion of liability for loss arising from the user's reliance on AI-generated content; indemnity clause; dispute resolution mechanism. (2) Require acceptance of Terms of Service before use (can be integrated into the existing consent gate). (3) Seek legal advice on the enforceability of limitation clauses in this context. (4) Note: under the Consumer Rights Act 2015, unfair terms in consumer contracts are not binding. Any limitation of liability must be fair, transparent, and prominently presented. |

---

### Risk 8.3: UK precedent for AI advice platform liability

| | |
|---|---|
| **Description** | There is limited but growing case law on AI advice platform liability. The legal landscape is evolving rapidly. |
| **Likelihood** | N/A (contextual risk) |
| **Impact** | N/A (contextual risk) |
| **Key precedents and developments** | (1) **Moffatt v. Air Canada** (2024 BCCRT 149, Canada): Company held liable for AI chatbot's inaccurate information. Tribunal rejected argument that chatbot was a "separate entity." User had no duty to cross-check information. While Canadian, the reasoning aligns with UK negligence principles. (2) **Mata v. Avianca, Inc.** (S.D.N.Y. 2023, USA): Lawyers sanctioned $5,000 for submitting AI-fabricated case citations. Established that AI hallucination of legal references is a concrete risk requiring human verification. (3) **UK AI Regulation Bill (2025)**: Private Members' Bill re-introduced to the House of Lords in March 2025, proposing a new "AI Authority" regulatory body. Not yet enacted. (4) **Data (Use and Access) Act 2025**: Relaxes some restrictions on automated decision-making but requires safeguards including meaningful human intervention. (5) **SRA authorisation of Garfield.Law (May 2025)**: First AI-based law firm authorised by SRA. Required strict safeguards including preventing AI from proposing case law, and named solicitors remain accountable. (6) **House of Lords committee (2024)**: Flagged that businesses using AI may be "on the hook" for risks they cannot fully understand. Bank of England survey found 46% of firms had only "partial understanding" of AI technologies they use. |
| **Assessment** | No direct UK precedent exists for liability of an AI complaints advice tool. However, the trajectory of case law and regulation strongly suggests that: (a) the operator will be held liable for the AI's outputs, not Google; (b) disclaimers alone may not be sufficient; (c) the standard of care will require reasonable verification of AI outputs; and (d) special category health data processing will attract heightened regulatory scrutiny. |

---

## Risk Summary Matrix

| # | Risk | Likelihood | Impact | Priority |
|---|------|-----------|--------|----------|
| 1.1 | Wrong complaint pathway | Medium | High | **High** |
| 1.2 | Wrong time limits | Low-Medium | Critical | **High** |
| 1.3 | Fabricated letter details | Medium | High | **High** |
| 1.4 | Wrong legislation cited | Low-Medium | Medium-High | **Medium** |
| 1.5 | Wrong rights information | Medium | High | **High** |
| 1.6 | Insufficient disclaimers | Medium | High | **High** |
| 1.7 | SRA/Law Society scrutiny | Low | Medium | **Low** |
| 2.1 | Gemini API data breach | Medium | Critical | **Critical** |
| 2.2 | localStorage data exposure | High | High | **Critical** |
| 2.3 | Breach notification failure | Medium | Critical | **High** |
| 2.4 | Unencrypted email transmission | Medium | Medium-High | **Medium** |
| 3.1 | User believes tool reported crime | Low | Critical | **High** |
| 3.2 | Inadequate safeguarding signposting | Medium | High | **High** |
| 4.1 | Unauthorised legal services | Low | Medium | **Low** |
| 4.2 | Information vs. advice blurring | Medium | Medium | **Medium** |
| 4.3 | No quality accreditation | Medium | Low-Medium | **Low** |
| 5.1 | WCAG non-compliance | Medium-High | High | **High** |
| 5.2 | Inaccessible to non-text users | High | Medium-High | **High** |
| 5.3 | PSED implications | Medium | Medium-High | **Medium** |
| 6.1 | Gemini API downtime | Medium | High | **High** |
| 6.2 | API cost overruns | Medium | Medium | **Medium** |
| 6.3 | Free API terms changes | Medium | Low-Medium | **Low** |
| 6.4 | Gemini discontinued | Medium | High | **Medium** |
| 6.5 | Railway hosting reliability | Low-Medium | Medium | **Low** |
| 7.1 | Negative media coverage | Medium-High | High | **High** |
| 7.2 | Complaint rejected/causes problems | Medium | Medium-High | **Medium** |
| 7.3 | Perceived as replacing human advice | Medium | Medium | **Medium** |
| 8.1 | No PI insurance | High | High | **Critical** |
| 8.2 | No Terms of Service | High | High | **Critical** |
| 8.3 | Evolving AI liability law | N/A | N/A | **Watch** |

---

## Priority Actions (Pre-Launch)

The following actions should be completed before the tool is made publicly available:

### Critical (Must do before launch)

1. **Upgrade to paid Gemini API tier** -- Google's terms require paid services for UK-serving applications. Free-tier data may be used for training and reviewed by humans. Accept the Cloud Data Processing Addendum.
2. **Draft and implement Terms of Service** -- Including limitation of liability, disclaimer that the tool is not legal advice, and user responsibility for verifying AI outputs.
3. **Obtain insurance** -- Professional indemnity insurance (declaring AI usage) and cyber liability insurance.
4. **Register with the ICO** and pay the data protection fee.
5. **Implement client-side encryption** for localStorage data.
6. **Complete the DPIA** -- Fill in all "[INSERT]" fields, including data controller details.

### High Priority (Should do before or shortly after launch)

7. **Add explicit AI accuracy disclaimers** on every screen: "AI can make mistakes. Check all information before acting on it."
8. **Add mandatory letter review step** -- Require the user to confirm they have reviewed the letter before enabling download/send.
9. **Commission a WCAG 2.2 AA accessibility audit**.
10. **Create a breach response plan** with documented procedures for ICO notification.
11. **Add rate limiting** to the API proxy.
12. **Add adversarial safeguarding testing** -- Test with abuse/crime scenarios to verify AI responses.
13. **Add nation-specific safeguarding contacts** for Wales, Scotland, and Northern Ireland.
14. **Implement an API fallback** -- Static pathway information when Gemini is unavailable.

### Medium Priority (Within first quarter)

15. Engage with the SRA innovation team and/or AdviceUK about the tool's approach.
16. Conduct an Equality Impact Assessment.
17. Add Easy Read content and large text mode.
18. Implement API cost monitoring and budget alerts.
19. Abstract the AI layer for provider portability.
20. Prepare a media response plan.

### Low Priority / Ongoing

21. Seek AQS or AdviceUK accreditation.
22. Periodic human review of AI response quality.
23. Monitor evolving AI regulation (AI Authority Bill, Data Use and Access Act 2025).
24. Test with users with accessibility needs.
25. Build partnerships with advice sector organisations.

---

## Sources

### Case Law
- [Mata v. Avianca, Inc. -- Wikipedia](https://en.wikipedia.org/wiki/Mata_v._Avianca,_Inc.)
- [Mata v. Avianca -- Seyfarth Shaw update on sanctions](https://www.seyfarth.com/news-insights/update-on-the-chatgpt-case-counsel-who-submitted-fake-cases-are-sanctioned.html)
- [Moffatt v. Air Canada -- ABA analysis](https://www.americanbar.org/groups/business_law/resources/business-law-today/2024-february/bc-tribunal-confirms-companies-remain-liable-information-provided-ai-chatbot/)
- [Moffatt v. Air Canada -- McCarthy Tetrault](https://www.mccarthy.ca/en/insights/blogs/techlex/moffatt-v-air-canada-misrepresentation-ai-chatbot)

### Regulatory Guidance
- [SRA Risk Outlook: AI in the Legal Market](https://www.sra.org.uk/sra/research-publications/artificial-intelligence-legal-market/)
- [SRA authorises first AI-based law firm](https://www.financialinstitutionslegalsnapshot.com/2025/05/08/uks-solicitors-regulatory-authority-authorises-first-purely-ai-based-firm/)
- [Legal Services Act 2007 -- Reserved Activities](https://legalservicesboard.org.uk/enquiries/frequently-asked-questions/reserved-legal-activities)
- [ICO Personal Data Breaches Guide](https://ico.org.uk/for-organisations/report-a-breach/personal-data-breach/personal-data-breaches-a-guide/)
- [ICO UK GDPR breach reporting guidance](https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/personal-data-breaches/)
- [Understanding UK accessibility requirements](https://www.gov.uk/guidance/accessibility-requirements-for-public-sector-websites-and-apps)

### ICO Enforcement Actions
- [Advanced Computer Software Group fined GBP 3m](https://www.pinsentmasons.com/out-law/news/nhs-processor-fined-ransomware-data-breach)
- [ICO fines Capita GBP 14 million](https://www.insideprivacy.com/data-privacy/ico-fines-capita-14-million-over-2023-data-breach/)
- [ICO enforcement trends 2025](https://measuredcollective.com/ico-enforcement-in-2025-record-fines-and-what-they-mean/)
- [Analysis of ICO fines 2024](https://www.urmconsulting.com/blog/analysis-of-fines-imposed-by-the-information-commissioners-office-in-2024)

### AI Security Incidents
- [Google patches Gemini enterprise flaw (GeminiJack)](https://www.cuinfosecurity.com/google-patches-ai-flaw-that-turned-gemini-into-spy-a-30236)
- [Gemini AI data leak 2024](https://thecyberexpress.com/google-gemini-ai-data-leak/)
- [Gemini CLI breach vulnerability 2025](https://nhimg.org/gemini-cli-breach-vulnerability-allows-silent-code-execution)
- [Gemini API terms and data privacy 2025](https://redact.dev/blog/gemini-api-terms-2025)

### Insurance and Liability
- [Silent AI cover risks -- Kennedys Law](https://www.kennedyslaw.com/en/thought-leadership/article/2025/silent-ai-cover-the-unforeseen-risks-for-insurers/)
- [AI PI Insurance 2025 Guide -- White Oak](https://whiteoakuk.com/professional-indemnity-insurance-loans-ai-risk-2025/)
- [AI liability and insurance -- IAPP](https://iapp.org/news/a/how-ai-liability-risks-are-challenging-the-insurance-landscape)

### Advice Sector and AI
- [Caddy -- Citizens Advice AI copilot](https://ai.gov.uk/projects/caddy/)
- [Caddy -- Stanford Justice Innovation](https://justiceinnovation.law.stanford.edu/how-ai-is-augmenting-human-led-legal-advice-at-citizens-advice/)
- [AdviceUK AI Solutions](https://www.adviceuk.org.uk/ai-solutions/)
- [Advice Quality Standard](https://asauk.org.uk/advice-quality-standard/)

### Media Coverage and Public Debate
- [AI in UK benefits system -- The Conversation](https://theconversation.com/ai-was-supposed-to-make-the-uk-benefits-system-more-efficient-instead-its-brought-bias-and-hunger-245616)
- [UK welfare fraud AI -- Tech Monitor](https://www.techmonitor.ai/digital-economy/ai-and-automation/uk-welfare-fraud-ai-system-faces-criticism-over-bias-and-transparency)
- [UK AI governance and inequalities -- LSE](https://blogs.lse.ac.uk/inequalities/2025/07/15/ai-governance-and-inequalities/)
